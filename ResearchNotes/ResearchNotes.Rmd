---
title: "ResearchNotes"
author: "James Woods"
date: "07/24/2014"
output: html_document
---
Read in the data file created by the data manipulation

```{r}
load("~/Research/Forecast-Peak-With-Google-Trends/DataManipulation/WTrends")
```

Replicate the hour 19 experiment from the proof of concept.  First how well do the forecasts work.
```{r}
summary(lm(HE19~F19,data=WTrends))

```
Looks like they are a little high most of the times by 3,400 MW but they comove the right way.

Now to add just PA
```{r}
summary(lm(HE19~F19+PATrends,data=WTrends))

```
Try with something else

```{r}
summary(lm(HE19~F19+NewsTrends,data=WTrends))

```
Oh, that works too

```{r}
summary(lm(HE19~F19+GasTrends,data=WTrends))

```
Gas does not

```{r}
summary(lm(HE19~F19+TrafficTrends,data=WTrends))

```
Traffic does not
```{r}
summary(lm(HE19~F19+RestaurantTrends,data=WTrends))

```
Restaurant does not
```{r}
summary(lm(HE19~F19+MovieTrends,data=WTrends))

```
Snort...Movie does.  Cool place to be.


Lets just check a few other hours

```{r}
summary(lm(HE11~F11+PATrends,data=WTrends))

```


```{r}
summary(lm(HE05~F05+PATrends,data=WTrends))

```
Small in absolute terms

# Weighting the search terms

I need to weight the state weather search terms by population. These population estimates are from  "Annual Estimates of the Resident Population for the United States, Regions, States, and Puerto Rico: April 1, 2010 to July 1, 2013" (CSV). 2013 Population Estimates. United States Census Bureau, Population Division. December 30, 2013. Retrieved December 30, 2013.


Here is where it gets strange.  I can't get count, just an index normalized by state.  This then is a population weighted value of indexes and not a population weighted index.

```{r}
# DE 925,749
# Maryland  5,928,814	
# New Jersey  8,899,339	
# Ohio  11,570,808
# Pennsylvania  12,773,801
# Virginia  8,260,405
# West Virginia  1,854,304
# Kentucky  4,395,295	


WTrends$Weather<-((925749*WTrends$DETrends)+(4395295*WTrends$KYTrends)+(5928814*WTrends$MDTrends)+(8899339*WTrends$NJTrends)+(11570808*WTrends$OHTrends)+(12773801*WTrends$PATrends)+(8260405*WTrends$VATrends)+(1854304*WTrends$WVTrends))/(925749+4395295+5928814+8899339+11570808+12773801+8260405+1854304)        
      
summary(WTrends$Weather)

```
Note that the index is now attenuated.


# Model all hours
```{r}

HourModel<-function(hour){
  Hour<-formatC(hour, width=2, flag="0")
  as.formula(paste("HE",Hour,"~ F",Hour,"+Weather",sep='' ))}


lapply(1:24, FUN = function(x)  summary(lm(HourModel(x), data=WTrends)))

```

Quick summary is that Google trends is helpful in all hours of the day. It also looks like the forecasts are usually off by a large amount.  I should look at those individually.

```{r}

HourModelForecastCheck<-function(hour){
  Hour<-formatC(hour, width=2, flag="0")
  as.formula(paste("HE",Hour,"~ F",Hour,sep='' ))}


lapply(1:24, FUN = function(x)  summary(lm(HourModelForecastCheck(x), data=WTrends)))

```
The intercept terms are almost all negative and almost all significant.  That means that they are forecasting high consistently.

Checking if the forecasts comove well, i.e., confidence interval contains 1.

```{r}

HourModelForecastCheck<-function(hour){
  Hour<-formatC(hour, width=2, flag="0")
  as.formula(paste("HE",Hour,"~ F",Hour,sep='' ))}


lapply(1:24, FUN = function(x)  confint(lm(HourModelForecastCheck(x), data=WTrends)))

```

Looks like it comoves well during peak hours but over corrects, greater than 1 in the other hours.

Given this I need to compare not just the difference between actual and forecast but the difference between with trends data and statistically adjusted forecast.  It is just not fair.

I will compare with the full model and via drop forward cross validation.

#Compare sigma on three models

For these comparisons I have to make sure I am using equal sized datasets.  So only those with trend data.

```{r}
WTrendsLimited<-WTrends[!is.na(WTrends$Weather),]
```


Now create a function that compares the sigma of the two models.  I will call the one without Google trends data the statistically adjusted forecasts.

```{r}

# Gives percent reduction in sigma
CompareSigma<-function(x){
SA<-summary(lm(HourModelForecastCheck(x), data=WTrendsLimited))$sigma  
Google<-summary(lm(HourModel(x), data=WTrendsLimited))$sigma  

(SA-Google)/(SA)
}

lapply(1:24, FUN = CompareSigma)
mean(unlist(lapply(1:24, FUN = CompareSigma)))

```

Nice percent savings vs the statistically adjusted model -- about 6% reduction average by hour.

Now make a comparison to raw


```{r}

# Gives percent reduction in sigma
CompareRaw<-function(x){
SA<-sd(WTrendsLimited[,x]-WTrendsLimited[,30])  
Google<-summary(lm(HourModel(x), data=WTrendsLimited))$sigma  

(SA-Google)/(SA)
}

lapply(1:24, FUN = CompareRaw)
mean(unlist(lapply(1:24, FUN = CompareRaw)))

```
Man that is huge.  Damn huge.  I must be misunderstanding what they mean by forecast.

Time, to look at this from a drop forward point of view.

# Compare with drop forward crossvalidation

This will be a fairer comparison.  I will boot up each model with 15 observations that then look at how they predict the next day, updating the model each day.

