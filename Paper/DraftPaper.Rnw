\documentclass{article}


\usepackage{longtable}
\usepackage[margin=1in]{geometry}
\usepackage[section]{placeins}
\usepackage{cite}

\title{Improving Day Ahead Electricity Load Forecasts with Google Trends}
\author{Cameron Mulder\\ James Woods}

% \date{}
 

\begin{document}
\maketitle
\setkeys{Gin}{width=0.7\textwidth}


\begin{abstract}
Modern short term load forecasting has grown in analytically complexity and sophistication.  Day ahead forecasts now commonly use neural nets, Monte Carlo simulations and a wealth of historical data.  What they have not done is fully captured the sentiment and intentions of the people using the electricity.  This paper introduces Google Trend data, a summary of Google searches, as a way of capturing this sentiment and refining forecasts.  We show with drop all forward cross validation that this amendment decreases forecast uncertainty by approximately 5\% when compared to a statistically adjusted forecast and by over 50\% when compared to raw forecasts.
\end{abstract}

\SweaveOpts{concordance=TRUE}

\section{Introduction}


<<echo=FALSE,results=hide>>=
load("~/Research/Forecast-Peak-With-Google-Trends/DataManipulation/WTrends")

WTrends$Weather<-((925749*WTrends$DETrends)+(4395295*WTrends$KYTrends)+(5928814*WTrends$MDTrends)+(8899339*WTrends$NJTrends)+(11570808*WTrends$OHTrends)+(12773801*WTrends$PATrends)+(8260405*WTrends$VATrends)+(1854304*WTrends$WVTrends))/(925749+4395295+5928814+8899339+11570808+12773801+8260405+1854304)  
@

\begin{enumerate}
  \item Intro to short term load forecasting.
  \item Why crowd sourced, non technical,  information could be useful.
  \item Google trends is the summation of Google searches.
  \item Outline of paper
\end{enumerate}


\section{Data Sources}

  \subsection{PJM Load Forecasts and Actuals}
  
  
<<echo=false,results=hide>>=
HourModelForecastCheck<-function(hour){
  Hour<-formatC(hour, width=2, flag="0")
  as.formula(paste("HE",Hour,"~ F",Hour,sep='' ))}


SAResults<-lapply(1:24, FUN = function(x)  lm(HourModelForecastCheck(x), data=WTrends))

@





\begin{figure}
\begin{center}
\caption{Confidence Intervals for Intercept Statistically Adjusted Models (95\%)}
<<echo=false,fig=TRUE>>=
library(ggplot2)
LowInter<-unlist(lapply(SAResults, FUN= function(x) confint(x) [1,1]))
HighInter<-unlist(lapply(SAResults, FUN= function(x) confint(x) [1,2]))
InterceptResults<-data.frame(1:24,LowInter,HighInter)
names(InterceptResults)[1]<-"Hour"

ggplot(InterceptResults, aes(Hour))+geom_pointrange(aes(y=LowInter, ymin=LowInter,ymax=HighInter))+geom_abline(intercept = 0)+ylab("Intercept Confidence Interval")
@
\end{center}
\end{figure}



\begin{figure}
\begin{center}
\caption{Confidence Intervals for Co-Movement Statistically Adjusted Models (95\%)}
<<echo=false,fig=TRUE>>=
library(ggplot2)
LowInter<-unlist(lapply(SAResults, FUN= function(x) confint(x) [2,1]))
HighInter<-unlist(lapply(SAResults, FUN= function(x) confint(x) [2,2]))
InterceptResults<-data.frame(1:24,LowInter,HighInter)
names(InterceptResults)[1]<-"Hour"

ggplot(InterceptResults, aes(Hour))+geom_pointrange(aes(y=LowInter, ymin=LowInter,ymax=HighInter))+geom_abline(intercept = 1,slope=0)+ylab("Intercept Confidence Interval")
@
\end{center}
\end{figure}




    \begin{enumerate}
      \item Data sources.
      \item Documentation of forecasting.
      \item Forecast bias
      \item Statistically adjusted forecasts.
      \item Note that almost all hours are biased and that co-movements are good for peak hours
    \end{enumerate}

  \subsection{Google Trends}

\begin{figure}
\begin{center}
\caption{State Weather Trends Indexes Over Time}
<<echo=false,fig=TRUE>>=
library(ggplot2)
library(reshape2)

JustWeather<-data.frame(WTrends$Date, WTrends$DETrends,WTrends$KYTrends, WTrends$MDTrends, WTrends$NJTrends, WTrends$OHTrends, WTrends$PATrends, WTrends$VATrends, WTrends$WVTrends, WTrends$Weather)

names(JustWeather)<-c("Date","DE","KY","MD","NJ","OH","PA","VA","WV","Weather")

MeltedTrends<-melt(JustWeather,variable.name="Index",id.var="Date",na.rm=TRUE)

ggplot(MeltedTrends, aes(Date, value,group= Index))+geom_line() +ylab("Index")+facet_grid(Index~.)

@
\end{center}
\end{figure}





\begin{figure}
\begin{center}
\caption{Trends Indexes Over Time}
<<echo=false,fig=TRUE>>=
library(ggplot2)
library(reshape2)

JustTrends<-data.frame(WTrends$Date, WTrends$GasTrends,WTrends$MovieTrends, WTrends$NewsTrends, WTrends$RestaurantTrends, WTrends$TrafficTrends,WTrends$Weather)
names(JustTrends)<-c("Date","Gas","Movie","News","Restaurant","Traffic","Weather")


MeltedTrends<-melt(JustTrends,variable.name="Index",id.var="Date",na.rm=TRUE)

ggplot(MeltedTrends, aes(Date, value,group= Index))+geom_line() +ylab("Index")+facet_grid(Index~.)

@
\end{center}
\end{figure}



    \begin{enumerate}
      \item Where to get the data
      \item Limitations
      \item Forming a population weighted index.
      \item Other common searches that will be used as counter examples.
    \end{enumerate}

\section{Post Forecast Addition of Google Trends Data}

  \begin{enumerate}
    \item Simple hourly models with Trends.
    \item Gross comparison with actual forecast and statistically adjusted forecasts.
    \item Why this is insufficient.
  \end{enumerate}

<<echo=false,results=hide>>=
WTrendsLimited<-WTrends[!is.na(WTrends$Weather),]


HourModel<-function(hour){
  Hour<-formatC(hour, width=2, flag="0")
  as.formula(paste("HE",Hour,"~ F",Hour,"+Weather",sep='' ))}


# Gives percent reduction in sigma
CompareRaw<-function(x){
SA<-sd(WTrendsLimited[,x]-WTrendsLimited[,30])  
Google<-summary(lm(HourModel(x), data=WTrendsLimited))$sigma  

(SA-Google)/(SA)
}

DirectCompare<-lapply(1:24, FUN = CompareRaw)
mean(unlist(lapply(1:24, FUN = CompareRaw)))

@



<<echo=false,results=hide>>=
HourModel<-function(hour){
  Hour<-formatC(hour, width=2, flag="0")
  as.formula(paste("HE",Hour,"~ F",Hour,"+Weather",sep='' ))}


GrossTrends<-lapply(1:24, FUN = function(x)  lm(HourModel(x), data=WTrends))
@


  
<<echo=false,results=hide>>=

# Gives percent reduction in sigma
CompareSigma<-function(x){
SA<-summary(lm(HourModelForecastCheck(x), data=WTrendsLimited))$sigma  
Google<-summary(lm(HourModel(x), data=WTrendsLimited))$sigma  

(SA-Google)/(SA)
}

ModelCompare<-lapply(1:24, FUN = CompareSigma)
mean(unlist(lapply(1:24, FUN = CompareSigma)))

@






  \subsection{Drop Forward Cross-validation}

<<echo=false,results=hide>>=

OneForwardError<-function(formula, nData){
  model<-lm(formula, data=WTrendsLimited[1:nData,], y=TRUE)
  modelPlus1<-lm(formula, data=WTrendsLimited[1:(nData+1),], y=TRUE)
  predict(model,WTrendsLimited[1+nData,])-tail(modelPlus1$y,1)
}

CVDropForward<-function(model){sd(unlist(lapply(20:(length(WTrendsLimited)-1), FUN= function(x) OneForwardError(model,x))))}

CompareSigmaCV<-function(x){
SA<-CVDropForward(HourModelForecastCheck(x))  
Google<-CVDropForward(HourModel(x))  

(SA-Google)/(SA)
}

CVImprove<-lapply(1:24, FUN = CompareSigmaCV)
CVImprove
mean(unlist(CVImprove))
@

  
<<echo=FALSE,results=tex>>=
ForCompareTable<-data.frame(1:24,unlist(DirectCompare)*100,unlist(ModelCompare)*100,unlist(CVImprove)*100)
names(ForCompareTable)<-c("Hour","Direct","Statistically Adjusted (Raw)", "Statistically Adjusted (CV)")
library(stargazer)
stargazer(ForCompareTable,summary=FALSE, title="Improvement in Forecasts Relative to Gross,  Statistically Adjusted, Drop Forward CV (Percent)",rownames=FALSE)
@
  
  
  
    \begin{enumerate}
      \item Cross validation concepts.
      \item Why drop forward cross validation is the right concept.
      \item Comparison of drop forward statistically adjusted and Trends adjusted with gross comparisons.
      \item Reiteration that comparison with raw forecasts is a slam dunk.
    \end{enumerate}
    
  \subsection{Counter-factual Test with Other Common Google Searches}
  
  
  
    \begin{enumerate}
      \item Comparison with: news, recipe, traffic, gas.
      \item Note that some of them kinda work.
    \end{enumerate}
    
<<echo=false,results=tex>>=
News<-lm(HE19~F19+NewsTrends,data=WTrends)
# Oh, that works too
Gas<-lm(HE19~F19+GasTrends,data=WTrends)
# Gas does not
Traffic<-lm(HE19~F19+TrafficTrends,data=WTrends)
# Traffic does not
Restaurant<-lm(HE19~F19+RestaurantTrends,data=WTrends)
# Restaurant does not
Movie<-lm(HE19~F19+MovieTrends,data=WTrends)
# Snort...Movie does.  Cool place to be.
library(stargazer)
stargazer(News,Gas,Traffic,Restaurant,Movie,title="Alternate Google Search Models for Hour 19",column.labels=c("News","Gas","Traffic","Restaurant","Movie"), dep.var.caption="Hour 19 Load",dep.var.labels.include = FALSE)
@



\section{Summary and Conclusions}


\appendix

\section{Hourly Models with Weather Searches}

<<echo=false,results=tex>>=

for(x in 1:12){
  stargazer(GrossTrends[[x]], title=paste("Hour",x), dep.var.labels=paste("Hour",x), covariate.labels=c("Forecast","Weather","Constant") )
}
@
\clearpage

<<echo=false, results=tex>>=

for(x in 13:24){
  stargazer(GrossTrends[[x]], title=paste("Hour",x), dep.var.labels=paste("Hour",x), covariate.labels=c("Forecast","Weather","Constant") )
}
@


% \bibliography{Trends.bib}
% \bibliographystyle{apalike}

\end{document}